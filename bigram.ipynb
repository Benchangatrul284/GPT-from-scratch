{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "if not os.path.exists('input.txt'):\n",
    "    print('Downloading input.txt...')\n",
    "    url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "    r = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 32 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "max_iters = 3000\n",
    "eval_interval = 300\n",
    "learning_rate = 1e-2\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Character based tokenizer, we encode the characters into integers.\n",
    "We decode the intergers back to characters to get the original text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocab size of this tokenizer is 65\n",
      "The encoding is [37, 53, 59, 1, 39, 56, 43, 1, 39, 1, 42, 53, 59, 41, 46, 1, 40, 39, 45, 2]\n"
     ]
    }
   ],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "# vocab_size\n",
    "print(f'The vocab size of this tokenizer is {len(stoi)}')\n",
    "# example\n",
    "ex = 'You are a douch bag!'\n",
    "print(f'The encoding is {encode(ex)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spilt into training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we randomly sample a batch of text from the training set and use it to train the model. The model will predict the next character in the sequence.   \n",
    "Since this is simply a classification problem, we will use the predicted character to calculate the cross entropy loss and update the model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    # The input (x) is a string of characters.\n",
    "    # The output (y) is the next character in the string.\n",
    "    # ix is the random index of a character in the string.\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We caclulate the average loss over eval_iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is only composed of an embedding table.  \n",
    "![embed.png](images/embed.png)  \n",
    "idx is a input to the embedding model (embedding table) , its dimension is (B,T) where each element is an integer.  \n",
    "logits is the output of the model (embedding table), its dimension is (B,T,D) where D is the embedding dimension.  \n",
    "target is the target of the model, its dimension is (B,T) where each element is an integer.  \n",
    "The loss is calculated by comparing the logits with the target.  \n",
    "\n",
    "## generation\n",
    "We generate token by token, we feed the model with the previous token to get the next token.\n",
    "We then concatenate the next token to the previous token and feed it back to the model to get the next token.\n",
    "Since the previous output is fed back to current input, this type of model is called \"autoregressive\" model.  \n",
    "![generate.png](images/generate.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module):\n",
    "    '''\n",
    "    A very simple language model that only looks at the previous one token.\n",
    "    '''\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(num_embeddings=vocab_size,embedding_dim=vocab_size)\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,D)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, D = logits.shape\n",
    "            logits = logits.view(B*T, D)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.7103, val loss 4.6829\n",
      "step 300: train loss 2.8052, val loss 2.8130\n",
      "step 600: train loss 2.5404, val loss 2.5631\n",
      "step 900: train loss 2.5104, val loss 2.5094\n",
      "step 1200: train loss 2.4763, val loss 2.5122\n",
      "step 1500: train loss 2.4777, val loss 2.4960\n",
      "step 1800: train loss 2.4733, val loss 2.4897\n",
      "step 2100: train loss 2.4688, val loss 2.4961\n",
      "step 2400: train loss 2.4632, val loss 2.4895\n",
      "step 2700: train loss 2.4621, val loss 2.4882\n"
     ]
    }
   ],
   "source": [
    "model = LanguageModel(vocab_size)\n",
    "m = model.to(device)\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "GLENor ath-ld,\n",
      "BUCA t t t\n",
      "\n",
      "kind er wodin:\n",
      "\n",
      "G t Ist h meancof y ht athoup.\n",
      "CEO rerd trd cis iss od fan IO:\n",
      "s pifrreroulere noade bulemeis e st stoffendou tinthoubrayod upeae sin t:\n",
      "Wire ayo neem at d,\n",
      "Whine jT:\n",
      "S:\n",
      "CLe bererdst heat ane bus woryou wingr t, I n:\n",
      "UEd;\n",
      "AN t e d b gstos?\n",
      "O:\n",
      "Th be sonoune o:\n",
      "PEENad om tthe tcisonas d chare bidrssupou g fre thyo t y I'ne the d, thaseanckead'dla w pelow y Ty t he, thereeegitos marous thardsisbr ove?\n",
      "\n",
      "AUS res amit heand k ialanit d, torwe ld ng tce y f l\n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
