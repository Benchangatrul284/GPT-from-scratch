{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B: batch dimension  \n",
    "T: sequence length  \n",
    "D: feature dimension (hidden_size)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "# B: batch dimension, T: sequence length, D: feature dimension (hidden size)\n",
    "B,T,D = 4,8,2\n",
    "x = torch.randn(B,T,D)\n",
    "xbow = torch.zeros((B,T,D))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1,:] #(T,D)\n",
    "        xbow[b,t,:] = torch.mean(xprev,dim=0) # reduce the time dimension\n",
    "\n",
    "x.shape == xbow.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We average the previous and current tokens.  \n",
    "![image](images/bow.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first batch of x is \n",
      "tensor([[ 0.1808, -0.0700],\n",
      "        [-0.3596, -0.9152],\n",
      "        [ 0.6258,  0.0255],\n",
      "        [ 0.9545,  0.0643],\n",
      "        [ 0.3612,  1.1679],\n",
      "        [-1.3499, -0.5102],\n",
      "        [ 0.2360, -0.2398],\n",
      "        [-0.9211,  1.5433]])\n",
      "The first batch of xbow is \n",
      "tensor([[ 0.1808, -0.0700],\n",
      "        [-0.0894, -0.4926],\n",
      "        [ 0.1490, -0.3199],\n",
      "        [ 0.3504, -0.2238],\n",
      "        [ 0.3525,  0.0545],\n",
      "        [ 0.0688, -0.0396],\n",
      "        [ 0.0927, -0.0682],\n",
      "        [-0.0341,  0.1332]])\n"
     ]
    }
   ],
   "source": [
    "print(f'The first batch of x is \\n{x[0]}')\n",
    "print(f'The first batch of xbow is \\n{xbow[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use matrix multiplication to calculate the mean of the previous tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "b=\n",
      "tensor([[8., 6.],\n",
      "        [5., 2.],\n",
      "        [4., 4.]])\n",
      "c=\n",
      "tensor([[8.0000, 6.0000],\n",
      "        [6.5000, 4.0000],\n",
      "        [5.6667, 4.0000]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tril(torch.ones(3,3)) # lower traingular matrix\n",
    "a = a / torch.sum(a,dim=1,keepdim=True) # sum up the rows\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "print(f'a=\\n{a}')\n",
    "print(f'b=\\n{b}')\n",
    "print(f'c=\\n{c}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn matrix: \n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n",
      "The first batch of x is \n",
      "tensor([[ 0.1808, -0.0700],\n",
      "        [-0.3596, -0.9152],\n",
      "        [ 0.6258,  0.0255],\n",
      "        [ 0.9545,  0.0643],\n",
      "        [ 0.3612,  1.1679],\n",
      "        [-1.3499, -0.5102],\n",
      "        [ 0.2360, -0.2398],\n",
      "        [-0.9211,  1.5433]])\n",
      "The first batch of xbow2 is \n",
      "tensor([[ 0.1808, -0.0700],\n",
      "        [-0.0894, -0.4926],\n",
      "        [ 0.1490, -0.3199],\n",
      "        [ 0.3504, -0.2238],\n",
      "        [ 0.3525,  0.0545],\n",
      "        [ 0.0688, -0.0396],\n",
      "        [ 0.0927, -0.0682],\n",
      "        [-0.0341,  0.1332]])\n"
     ]
    }
   ],
   "source": [
    "# a stands for attention matrix\n",
    "# b stands for input matrix\n",
    "# c stands for output matrix\n",
    "attn = torch.tril(torch.ones(T,T))\n",
    "attn = attn / torch.sum(attn,dim=1,keepdim=True) # sum up the rows\n",
    "print(f'attn matrix: \\n{attn}')\n",
    "xbow2 = attn @ x #(T,T) @ (B,T,D) -> (B,T,D)\n",
    "print(f'The first batch of xbow2 is \\n{xbow2[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use softmax to produce the attention weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn matrix: \n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n",
      "The first batch of xbow3 is \n",
      "tensor([[ 0.1808, -0.0700],\n",
      "        [-0.0894, -0.4926],\n",
      "        [ 0.1490, -0.3199],\n",
      "        [ 0.3504, -0.2238],\n",
      "        [ 0.3525,  0.0545],\n",
      "        [ 0.0688, -0.0396],\n",
      "        [ 0.0927, -0.0682],\n",
      "        [-0.0341,  0.1332]])\n"
     ]
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T,T))\n",
    "attn = torch.zeros(T,T)\n",
    "attn = attn.masked_fill(tril == 0, float('-inf')) # mask out the upper triangle (masked-attention)\n",
    "attn = F.softmax(attn,dim=-1) # reduce the time dimension\n",
    "xbow3 = attn @ x\n",
    "print(f'attn matrix: \\n{attn}')\n",
    "print(f'The first batch of xbow3 is \\n{xbow3[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previous attention matrix simply averages the previous tokens and current tokens.  \n",
    "Can we modify the weight matrix to pay more attention to specific token?  \n",
    "Yes! We can use self-attention machanism to do so.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention matrix: \n",
      "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.8203, 0.1797, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3286, 0.3266, 0.3448, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3526, 0.0940, 0.5211, 0.0323, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.8307, 0.0526, 0.0739, 0.0089, 0.0339, 0.0000, 0.0000, 0.0000],\n",
      "         [0.1564, 0.0692, 0.2802, 0.2136, 0.1546, 0.1260, 0.0000, 0.0000],\n",
      "         [0.2718, 0.0714, 0.4514, 0.1176, 0.0164, 0.0538, 0.0176, 0.0000],\n",
      "         [0.0171, 0.0574, 0.1648, 0.3710, 0.0882, 0.0989, 0.1188, 0.0838]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "B,T,D = 1,8,32\n",
    "x = torch.randn(B,T,D)\n",
    "\n",
    "head_size = 16\n",
    "key = nn.Linear(D, head_size)\n",
    "query = nn.Linear(D, head_size)\n",
    "\n",
    "k = key(x) # (B,T,D) -> (B,T,head_size)\n",
    "q = query(x) # (B,T,D) -> (B,T,head_size)\n",
    "\n",
    "# previous, we simply set weight matrix to be all zeros\n",
    "attn = k @ q.transpose(2,1) # (B,T,head_size) @ (B,head_size,T) -> (B,T,T)\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "attn = attn.masked_fill(tril == 0, float('-inf')) # mask out the upper triangle (masked-attention)\n",
    "attn = F.softmax(attn,dim=-1)\n",
    "print(f'attention matrix: \\n{attn}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, we do dot product between attention weights and the embedding of the tokens. \n",
    "Here, we perform dot product between the attention weights and \"values\" of the tokens.  The value is produced by passing the embedding through a linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 16])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value = nn.Linear(D,head_size)\n",
    "v = value(x) # (B,T,D) -> (B,T,head_size)\n",
    "o = attn @ v # (B,T,T) @ (B,T,head_size) -> (B,T,head_size)\n",
    "o.shape # (B,T,head_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to recover the dimension of the output, we pass the dot product through a linear layer called output layer. This layer is optional since nowadays the dimension of dot product between weight matrix and value matrix is designed to be the same as the input dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the input dimension is torch.Size([1, 8, 32]) and the output dimension is torch.Size([1, 8, 32])\n"
     ]
    }
   ],
   "source": [
    "out = nn.Linear(head_size,D)\n",
    "o = out(o) # (B,T,head_size) -> (B,T,D)\n",
    "o.shape # (B,T,head_size)\n",
    "print(f'the input dimension is {x.shape} and the output dimension is {o.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled product attention\n",
    "We expect the variance to be close to 1, but after dot product, the variance will be close to head_size. We therefore divide the dot product by sqrt(head_size) to scale the variance back to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The variance of k is 1.0172395706176758\n",
      "The variance of q is 0.9533872604370117\n",
      "The variance of attn is 18.86295509338379\n"
     ]
    }
   ],
   "source": [
    "k = torch.randn(B,T,head_size)\n",
    "q = torch.randn(B,T,head_size)\n",
    "print('The variance of k is', torch.var(k).item())\n",
    "print('The variance of q is', torch.var(q).item())\n",
    "attn = k @ q.transpose(2,1)\n",
    "print('The variance of attn is', torch.var(attn).item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The variance of attn is 1.0233604907989502\n"
     ]
    }
   ],
   "source": [
    "attn = k @ q.transpose(2,1) / (head_size**0.5)\n",
    "print('The variance of attn is', torch.var(attn).item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
