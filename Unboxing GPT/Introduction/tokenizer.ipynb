{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model_type = \"openai-community/gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer encodes string into tokens, and decode tokens into string.\n",
    "Tokenizer is model specific, so we need to use the same tokenizer that was used during training.\n",
    "The common parameter of the tokenizer is:\n",
    "* vocab_size: Number of tokens in the vocabulary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tokenizer has 50257 in the vocabulary.\n"
     ]
    }
   ],
   "source": [
    "print(f\"The tokenizer has {tokenizer.vocab_size} in the vocabulary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Special tokens:\n",
    "Some special tokens are added to the vocabulary:\n",
    "* bos_token: Beginning of sentence token. It is added at the beginning of the input sequence. GPT2 model don't have this special token.\n",
    "* eos_token: End of sentence token. When model generates this token, it stops generating further tokens.\n",
    "* unk_token: Unknown token. When model encounters a token that is not in the vocabulary, it replaces it with this token.\n",
    "* padding token: when training the language model, we expect the input sequences to be of the same length to form a batch. If the input sequence is shorter than the model_max_length, it is padded with this token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT2 only has eos_token as special token. It doesn't have bos_token, unk_token, and padding token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The eos_token is <|endoftext|>\n",
      "The id of the eos_token is 50256\n",
      "The last token of the encoded text is 50256\n"
     ]
    }
   ],
   "source": [
    "# eos_token (end of sentence token) is the token that is added to the end of the input text\n",
    "text = \"Hello,\"\n",
    "print(f\"The eos_token is {tokenizer.eos_token}\")\n",
    "print(f\"The id of the eos_token is {tokenizer.eos_token_id}\")\n",
    "tokens = tokenizer.encode(text + tokenizer.eos_token, add_special_tokens=True)\n",
    "print(\"The last token of the encoded text is\", tokens[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We change to another model which has bos_token, unk_token, and padding token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The bos_token is <s>\n",
      "The id of the bos_token is 1\n",
      "The first token of the encoded text is 1\n"
     ]
    }
   ],
   "source": [
    "# bos_token (begin of sentence token) is the token that is added to the beginning of the input text\n",
    "# https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
    "model_type = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_type)\n",
    "text = \"Hello,\"\n",
    "print(f\"The bos_token is {tokenizer.bos_token}\")\n",
    "print(f\"The id of the bos_token is {tokenizer.bos_token_id}\")\n",
    "tokens = tokenizer.encode(text)\n",
    "print(\"The first token of the encoded text is\", tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The unk_token is <unk>\n",
      "The id of the unk_token is 0\n",
      "The last token of the encoded text is 0\n"
     ]
    }
   ],
   "source": [
    "# unk_token (unknown token) is the token that is used when a token is not in the vocabulary\n",
    "text = \"Hello,\"\n",
    "print(f\"The unk_token is {tokenizer.unk_token}\")\n",
    "print(f\"The id of the unk_token is {tokenizer.unk_token_id}\")\n",
    "tokens = tokenizer.encode(text + tokenizer.unk_token, add_special_tokens=True)\n",
    "print(\"The last token of the encoded text is\", tokens[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The pad_token is </s>\n",
      "The id of the pad_token is 2\n",
      "The last token of the encoded text is 2\n"
     ]
    }
   ],
   "source": [
    "# pad_token (padding token) is the token that is used to pad the input text to the same length\n",
    "text = \"Hello,\"\n",
    "print(f\"The pad_token is {tokenizer.pad_token}\")\n",
    "print(f\"The id of the pad_token is {tokenizer.pad_token_id}\")\n",
    "tokens = tokenizer.encode(text + tokenizer.pad_token, add_special_tokens=True)\n",
    "print(\"The last token of the encoded text is\", tokens[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The eos_token is </s>\n",
      "The id of the eos_token is 2\n",
      "The last token of the encoded text is 2\n"
     ]
    }
   ],
   "source": [
    "# eos_token (end of sentence token) is the token that is added to the end of the input text\n",
    "text = \"Hello,\"\n",
    "print(f\"The eos_token is {tokenizer.eos_token}\")\n",
    "print(f\"The id of the eos_token is {tokenizer.eos_token_id}\")\n",
    "tokens = tokenizer.encode(text + tokenizer.eos_token, add_special_tokens=True)\n",
    "print(\"The last token of the encoded text is\", tokens[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found that the eos_token is identical to pad_token in the tokenizer. Since padding token is used to pad the input sequence, no tokens will appear after the padding token. So, we can use the padding token as eos_token.\n",
    "\n",
    "We can investigate the tokenizer in tokenizer_config.json file.  \n",
    "https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/blob/main/tokenizer_config.json"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
